## Web-Scraping

import requests
from bs4 import BeautifulSoup
from lxml import html
import pickle
import requests
import json

with open("/content/drive/MyDrive/commentary_urls.pickle", "rb") as file:
    urls = pickle.load(file)

urls_list = []

for url in urls:
    url_parameters = {}
    parameters = url.split('/')
    team_a, team_b = parameters[4].split('-vs-')
    url_code, id = parameters[5].split('#')
    print(team_a, team_b, url_code, id)

    url_parameters['team_a'] = team_a
    url_parameters['team_b'] = team_b
    url_parameters['url_code'] = url_code
    url_parameters['id'] = id

    urls_list.append(url_parameters)

urls_list

'https://www.fotmob.com/api/ltc?ltcUrl=data.fotmob.com/webcl/ltc/gsm/4506505_en.json.gz&teams=["Brighton+&+Hove+Albion","Arsenal"]'

curl 'https://www.fotmob.com/api/ltc?ltcUrl=data.fotmob.com%2Fwebcl%2Fltc%2Fgsm%2F4506505_en.json.gz&teams=%5B%22Brighton+%26+Hove+Albion%22%2C%22Arsenal%22%5D' \
  -H 'sec-ch-ua-platform: "macOS"' \
  -H 'Referer: https://www.fotmob.com/matches/arsenal-vs-brighton-hove-albion/3bfk5g' \
  -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36' \
  -H 'x-mas: eyJib2R5Ijp7InVybCI6Ii9hcGkvbHRjP2x0Y1VybD1kYXRhLmZvdG1vYi5jb20lMkZ3ZWJjbCUyRmx0YyUyRmdzbSUyRjQ1MDY1MDVfZW4uanNvbi5neiZ0ZWFtcz0lNUIlMjJCcmlnaHRvbislMjYrSG92ZStBbGJpb24lMjIlMkMlMjJBcnNlbmFsJTIyJTVEIiwiY29kZSI6MTc0MTg2OTQyNjY2OCwiZm9vIjoicHJvZHVjdGlvbjo1MTgzYjk4ZDlkNDM2NmRhZGM4NDZiNzE1ZDY0MGQxOWMwMzY2ZThiLXVuZGVmaW5lZCJ9LCJzaWduYXR1cmUiOiIzREY0QTQ0MEU1QUQ1NzQwMTZFMzE2NTg1RjNDNTZFMiJ9' \
  -H 'sec-ch-ua: "Brave";v="131", "Chromium";v="131", "Not_A Brand";v="24"' \
  -H 'sec-ch-ua-mobile: ?0'

def get_commentary(team_a, team_b, url_code, id):
    url = f"https://www.fotmob.com/api/ltc?ltcUrl=data.fotmob.com%2Fwebcl%2Fltc%2Fgsm%2F{id}_en.json.gz&teams=%5B%22{team_a.replace('-', '+')}%22%2C%22{team_b.replace('-', '+')}%22%5D" # &teams=%5B%22{team_a}%22%2C%22{team_b}%22%5D

    headers = {
        "sec-ch-ua-platform": '"macOS"',
        "Referer": f"https://www.fotmob.com/matches/{team_a}-vs-{team_b}/{url_code}",
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "x-mas": "eyJib2R5Ijp7InVybCI6Ii9hcGkvbHRjP2x0Y1VybD1kYXRhLmZvdG1vYi5jb20lMkZ3ZWJjbCUyRmx0YyUyRmdzbSUyRjQ1MDY2MTJfZW4uanNvbi5neiZ0ZWFtcz0lNUIlMjJXb2x2ZXJoYW1wdG9uK1dhbmRlcmVycyUyMiUyQyUyMkFyc2VuYWwlMjIlNUQiLCJjb2RlIjoxNzQxODQ3MjgwODQ2LCJmb28iOiJwcm9kdWN0aW9uOmI1MzIwODMyN2U5ZGQxOTcyM2VjNTEzM2ExYzQ3Y2Q1NDYwNzMyZWMtdW5kZWZpbmVkIn0sInNpZ25hdHVyZSI6Ijk1QjIxQkUzMEQwQThCMTIzNUQ3NzgxMkY2RjM2OTE5In0=",
        "sec-ch-ua": '"Brave";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
        "sec-ch-ua-mobile": "?0"
    }

    response = requests.get(url, headers=headers)

    # print(response.status_code)

    json_object = json.loads(response.text)

    return json_object["events"]

dataset_dict = {"Team A": [], "Team B": [], "URL Code": [], "ID": [], "Comment": [], "Type": [], "Players": [], "Minute": []}

for url_params in urls_list:
    team_a = url_params['team_a']
    team_b = url_params['team_b']
    url_code = url_params['url_code']
    id = url_params['id']

    data = get_commentary(team_a, team_b, url_code, id)

    for event in data:
        dataset_dict["Comment"].append(event["text"].lower())
        dataset_dict["Type"].append(event["type"])
        # print(event["time"])
        # print(type(event["time"]))
        if event["time"]:
            extra = int(event["time"]["added"][1:]) if event["time"]["added"] else 0
            dataset_dict["Minute"].append(int(event["time"]["main"][:-1]) + extra)
        else:
            dataset_dict["Minute"].append(None)

        if len(event["players"]) != 0:
            players = []
            # print(event["type"], event["text"], event["players"], sep="\n")
            for player in event["players"]:
                players.append(player["name"])
            dataset_dict["Players"].append(players)
        else:
            dataset_dict["Players"].append(event["players"])
            # print(event["type"], event["text"], sep="\n")

        dataset_dict["Team A"].append(team_a)
        dataset_dict["Team B"].append(team_b)
        dataset_dict["URL Code"].append(url_code)
        dataset_dict["ID"].append(id)

# Dataset Raw

dataset = pd.DataFrame(dataset_dict)

## Data Modification

!pip install rapidfuzz

def generate_url(team_a, team_b, url_code, id):
    return f'https://www.fotmob.com/matches/{team_a}-vs-{team_b}/{url_code}#{id}:tab=ticker'

def directional_partial_ratio(short, long_):

    if len(short) > len(long_):
        return fuzz.ratio(short, long_)
    else:
        return fuzz.partial_ratio(short, long_)

from rapidfuzz import fuzz, process

def name_in_text(player, comment, threshold):
    highest_ratio = 0
    word_match_index = 0

    # comment.replace("-", " ")
    comment_list = comment.split(' ')

    for name in player.split(' '):
        for word in comment_list:
            ratio = directional_partial_ratio(name.lower(), word)
            if ratio > highest_ratio:
                highest_ratio = ratio
                word_match_index = comment_list.index(word)

    if highest_ratio > threshold:
        return comment_list[word_match_index], highest_ratio, comment_list[word_match_index], name, comment
    else:
        return False, highest_ratio, comment_list[word_match_index], name, comment

dataset = pd.read_csv("/content/drive/MyDrive/commentary_final_dataset.csv")

dataset

# Remove comments that are outside the time line (that don't have time)
dataset.drop(index=dataset.index[dataset["Minute"].isnull()].tolist(), inplace=True)

# Remove comments in the irrelevant category
dataset.drop(index=dataset.index[dataset["Type"] == "post_match summary"], inplace=True)
dataset.drop(index=dataset.index[dataset["Type"] == "half_time summary"], inplace=True)

dataset['Player Name In Commentary'] = '[]'
for index, row in dataset[["Comment", "Players"]].iterrows():
    comment, players = row
    if players != '[]':
        players_list = eval(players)
        for player in players_list:
            name = name_in_text(player, comment, 60)[0]
            if name:
                name_ = eval(dataset["Player Name In Commentary"].iloc[index]) + [name.lower()]
                dataset.loc[index, "Player Name In Commentary"] = str(name_)

dataset_nodirection = dataset.query("`Players` != '[]' and `Player Name In Commentary` == '[]'")

dataset_direction = dataset.query("`Players` != '[]' and `Player Name In Commentary` == '[]'")

for index, row in dataset.query("`Players` != '[]' and `Player Name In Commentary` != '[]'").iterrows():
    # Team A	Team B	URL Code	ID	Comment	Type	Players	Minute	Player Name In Commentary
    team_a, team_b, url_code, id, comment, type_, players, minute, player_name_in_comment = row
    url = generate_url(team_a, team_b, url_code, id)
    print(url, end="\n")
    if players != '[]':
        players_list = eval(players)
        for player in players_list:
            # comment_list[word_match_index] / False, highest_ratio, comment_list[word_match_index], name, comment
            print(name_in_text(player, comment, 60)[1:5])

for index, row in pd.DataFrame(ds1.difference(ds2)).iterrows():
    # Team A	Team B	URL Code	ID	Comment	Type	Players	Minute	Player Name In Commentary
    team_a, team_b, url_code, id, comment, type_, players, minute, player_name_in_comment = row
    url = generate_url(team_a, team_b, url_code, id)
    print(url, end="\n")
    if players != '[]':
        players_list = eval(players)
        for player in players_list:
            # comment_list[word_match_index] / False, highest_ratio, comment_list[word_match_index], name, comment
            print(name_in_text(player, comment, 60)[1:5])

generate_url('everton', 'aston-villa', '2ykmb4', '4506294')

dataset['labels'], label_mapping = pd.factorize(dataframe['Type'])

# Save Changes
dataset.to_csv("/content/drive/MyDrive/commentary_final_dataset.csv", index=False)

## Data Inspection and Analysis

dataset = pd.read_csv("/content/drive/MyDrive/commentary_final_dataset.csv")

dataset.columns

dataset["Type"].unique()

dataset.loc[dataset["Players"] != '[]']["Type"].unique()

set(dataset["Type"].unique()).difference(set(dataset.loc[dataset["Minute"].astype(str) != 'nan']["Type"].unique()))

dataset.loc[dataset['Type'] == 'Y2C'][["Comment", "Team A", "Team B", "URL Code", "Minute"]].iloc[0]

dataset.query("`ID` == 4506283")

dataset["Comment"].iloc[15334:]

dataset.loc[dataset["URL Code"] == "3bfk5g"]

dataset["Type"].unique()

dataset.query("`Type` == 'stats' and `Minute`.notnull()")[["Comment", "Team A", "Team B", "URL Code", "ID"]].iloc[5]

dataset.query("`Minute`.isnull()")["Type"].unique()

team_a = 'tottenham-hotspur'
team_b = 'wolverhampton-wanderers'
url_code = '2fydv8'
id = '4506501'
f"https://www.fotmob.com/matches/{team_a}-vs-{team_b}/{url_code}#{id}:tab=ticker"

AS - Assist
SI - Substitution
G - Goal
YC - Yellow Card
RC - Red Card
var - Video Assistant Referee
Y2C - Yellow and Red Card
PM - Penealty Miss

Discard the following:
post_match summary
half_time summary

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(dataset["Comment"], dataset[["Player Name In Commentary", "Type"]], train_size=0.8, random_state=42)

pd.concat([X_train, y_train], axis=1).to_csv("Football_Commentary_train.csv", index=False)

pd.concat([X_test, y_test], axis=1).to_csv("Football_Commentary_test.csv", index=False)

## bert-base Fine Tuning

!pip install --upgrade transformers datasets evaluate huggingface_hub torch

from google.colab import userdata
HF_TOKEN = userdata.get('HF_TOKEN')

### Model training

from datasets import DatasetDict, Dataset, load_dataset
from transformers import (AutoTokenizer, AutoModelForSequenceClassification,
                         TrainingArguments, Trainer, DataCollatorWithPadding)
import evaluate
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split

dataframe = pd.read_csv("/content/drive/MyDrive/commentary_final_dataset.csv")

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
le.fit(list(dataframe["Type"].unique()))
dataframe["labels-m1"] = le.transform(dataframe["Type"])

from datasets import Dataset, DatasetDict

train, test = train_test_split(dataframe[["Comment", "labels-m1"]], test_size=0.2)
train = Dataset.from_pandas(train, preserve_index=False)
test = Dataset.from_pandas(test, preserve_index=False)

dataset_dict = DatasetDict({
    "train": train,
    "test": test
})

id2label = {}
label2id = {}

for id in range(17):
    label = str(le.inverse_transform([id])[0])
    id2label[id] = label
    label2id[label] = id

# define pre-trained model path
model_path = "google-bert/bert-base-uncased"

# load model tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path)

model = AutoModelForSequenceClassification.from_pretrained(model_path,
                                                           num_labels=17,
                                                           id2label=id2label,
                                                           label2id=label2id,)

# freeze all base model parameters
for name, param in model.base_model.named_parameters():
    param.requires_grad = False

# unfreeze base model pooling layers
for name, param in model.base_model.named_parameters():
    if "pooler" in name:
        param.requires_grad = True

# define text preprocessing
def preprocess_function(examples):
    # return tokenized text with truncation
    return tokenizer(examples["Comment"], truncation=True, padding='max_length', max_length=128)

# preprocess all datasets
tokenized_data = dataset_dict.map(preprocess_function, batched=True)

# create data collator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# load metrics
import evaluate
import numpy as np
from scipy.special import softmax

accuracy = evaluate.load("accuracy")
auc_score = evaluate.load("roc_auc")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred

    labels = labels.astype(np.int32)  # Fix: ensure correct dtype
    probabilities = softmax(predictions, axis=1).astype(np.float32)  # Fix: ensure float32

    # Predict most probable class
    predicted_classes = np.argmax(probabilities, axis=1)

    # Compute Accuracy
    acc = np.round(
        accuracy.compute(
            predictions=predicted_classes,
            references=labels
        )["accuracy"], 3
    )

    return {"Accuracy": acc}

# hyperparameters
lr = 2e-4
batch_size = 8
num_epochs = 10

training_args = TrainingArguments(
    output_dir="football_commentary-EE",
    learning_rate=lr,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_epochs,
    logging_strategy="epoch",
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data["train"],
    eval_dataset=tokenized_data["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

trainer.args.num_train_epochs = 27
trainer.args.learning_rate = 1e-5
trainer.train(resume_from_checkpoint=True)

### Training Log

training_log = pd.DataFrame(trainer.state.log_history)

training_log[["epoch", "eval_loss", "eval_Accuracy"]].dropna().style.hide()

### Save the model

trainer.model.save_pretrained("/content/my-bert-finetuned")
tokenizer.save_pretrained("/content/my-bert-finetuned")

import shutil

shutil.make_archive("/content/my-bert-finetuned", 'zip', "/content/my-bert-finetuned")

### Model testing

# prompt: import fine tuned model from hugging face transformers

from transformers import pipeline

# Replace with your model and tokenizer paths
model_path = "falahmanalodi/Football-Commentary-EE"

# Load the fine-tuned model and tokenizer
classifier = pipeline("text-classification", model=model_path, tokenizer=model_path)


# Example usage: classify some text
text = "Ronaldo Scores a fantastic goal"
result = classifier(text)

result

### New dataset preperation for Model 2 fine tuning

dataset = pd.read_csv("/content/drive/MyDrive/commentary_final_dataset.csv")

import pandas as pd
import random

dataset.columns

TRUNCATE_PERCENT = 0.4  # 50% of data will be used for truncation

# Sample 50% of original dataset to create Incomplete examples
truncate_samples = dataset.sample(frac=TRUNCATE_PERCENT, random_state=42).copy()

# Truncate each sampled comment by 40%â€“90% of its length
def truncate_text(text):
    words = text.strip().split()
    if len(words) <= 3:
        return text  # don't truncate very short ones
    keep_ratio = random.uniform(0.1, 0.9)
    keep_len = max(1, int(len(words) * keep_ratio))
    truncated = ' '.join(words[:keep_len])
    return truncated

truncate_text("Hello world, this is falah. im a cool")

truncate_samples["Comment"] = truncate_samples["Comment"].apply(truncate_text)

truncate_samples["Type-m2"] = "Incomplete"

final_data = pd.concat([dataset, truncate_samples], ignore_index=True)
final_data = final_data.sample(frac=1.0, random_state=42).reset_index(drop=True)

final_data.to_csv("/content/drive/MyDrive/commentary_final_dataset.csv", index=False)

dataset.loc[final_data["Type-m2"].isnull(), "Type-m2"] = "Complete"

dataset.to_csv("/content/drive/MyDrive/commentary_final_dataset.csv", index=False)

## microsoft/MiniLM-L6-H384-uncased - Fine tuning for finding semantically connected sentances

!pip install --upgrade transformers datasets evaluate huggingface_hub torch

from google.colab import userdata
HF_TOKEN = userdata.get('HF_TOKEN')

!huggingface-cli login

from datasets import DatasetDict, Dataset, load_dataset
from transformers import (AutoTokenizer, AutoModelForSequenceClassification,
                         TrainingArguments, Trainer, DataCollatorWithPadding)
import evaluate
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split

dataframe = pd.read_csv("/content/drive/MyDrive/commentary_final_dataset.csv")

dataframe.columns

dataframe.drop(columns=["labels-m1", "labels-m2"], inplace=True)

dataframe = load_dataset("falahmanalodi/Football-Commentary")

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
le.fit(list(dataframe["Type-m2"].unique()))
dataframe["labels-m2"] = le.transform(dataframe["Type-m2"])

le.transform(["Incomplete"]), le.transform(["Complete"])

len(le.classes_)

id2label = {}
label2id = {}

for id in range(len(le.classes_)):
    label = str(le.inverse_transform([id])[0])
    id2label[id] = label
    label2id[label] = id

from datasets import Dataset, DatasetDict

train, test = train_test_split(dataframe[["Comment", "labels-m2"]], test_size=0.2)
train = Dataset.from_pandas(train, preserve_index=False)
test = Dataset.from_pandas(test, preserve_index=False)

dataset_dict = DatasetDict({
    "train": train,
    "test": test
})

dataset_dict = dataset_dict.rename_column("labels-m2", "labels")

# define pre-trained model path
model_path = "microsoft/MiniLM-L12-H384-uncased"

# load model tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path)

model = AutoModelForSequenceClassification.from_pretrained(model_path,
                                                           num_labels=2,
                                                           id2label=id2label,
                                                           label2id=label2id,)

# freeze all base model parameters
for name, param in model.base_model.named_parameters():
    param.requires_grad = False

# unfreeze base model pooling layers
found_pooler = False
for name, param in model.base_model.named_parameters():
    if "pooler" in name:
        param.requires_grad = True
        found_pooler = True
if not found_pooler:
    print("âš ï¸ No pooler layer found â€” all base layers remain frozen.")

# define text preprocessing
def preprocess_function(examples):
    # return tokenized text with truncation
    return tokenizer(examples["Comment"], truncation=True, padding='max_length', max_length=128)

# preprocess all datasets
tokenized_data = dataset_dict.map(preprocess_function, batched=True)

# create data collator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# load metrics
import evaluate
import numpy as np
from scipy.special import softmax

accuracy = evaluate.load("accuracy")
auc_score = evaluate.load("roc_auc")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred

    labels = labels.astype(np.int32)  # Fix: ensure correct dtype
    probabilities = softmax(predictions, axis=1).astype(np.float32)  # Fix: ensure float32

    # Predict most probable class
    predicted_classes = np.argmax(probabilities, axis=1)

    # Compute Accuracy
    acc = np.round(
        accuracy.compute(
            predictions=predicted_classes,
            references=labels
        )["accuracy"], 3
    )

    return {"Accuracy": acc}

# hyperparameters
lr = 2e-5
batch_size = 16
num_epochs = 8

training_args = TrainingArguments(
    output_dir="football_commentary-comment-clusterer",
    learning_rate=lr,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_epochs,
    logging_strategy="epoch",
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data["train"],
    eval_dataset=tokenized_data["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

trainer.args.num_train_epochs = 20
trainer.train(resume_from_checkpoint=True)

trainer.model.cpu()
trainer.model.save_pretrained("/content/microsoft_MiniLM")
tokenizer.save_pretrained("/content/microsoft_MiniLM")

import shutil

shutil.make_archive("/content/microsoft_MiniLM", 'zip', "/content/microsoft_MiniLM")

### Testing model

bert_model = AutoModelForSequenceClassification.from_pretrained("falahmanalodi/Football-Commentary-EE")
tokenizer = AutoTokenizer.from_pretrained("falahmanalodi/Football-Commentary-EE")

def predict(comment_):
    inputs = tokenizer(comment_, return_tensors="pt", truncation=True)
    outputs = bert_model(**inputs)
    predicted_class = outputs.logits.argmax(dim=1).item()
    print(id2label[int(predicted_class)])

import time

start = time.perf_counter()
predict("brilliant pass from rice and a good save from martinez!! the ball finds its way back to rice on the left wing after the corner and he holds onto it, waiting for a run. he spots martinelli in space on the far post and clips in a lovely cross. the angle isn't clean but the brazilian still gets a good touch on it to force martinez into making a neat save.")
stop = time.perf_counter()

stop - start

microsoft_model = AutoModelForSequenceClassification.from_pretrained("/content/microsoft_MiniLM")
tokenizer = AutoTokenizer.from_pretrained("/content/microsoft_MiniLM")

def predict(comment_):
    inputs = tokenizer(comment_, return_tensors="pt", truncation=True)
    outputs = microsoft_model(**inputs)
    predicted_class = outputs.logits.argmax(dim=1).item()
    print(id2label[int(predicted_class)])

import time

start = time.perf_counter()
predict("palace come flying out of the blocks to put pressure on the hosts early. it is perhaps a little too intense as gillett gives a foul to chelsea deep in their own half.	")
stop = time.perf_counter()
print(stop - start)

## BERT-BASE-NER Fine Tuning

### Data pre-processing

dataset = load_dataset("falahmanalodi/Football-Commentary")

dataframe = dataset["train"].to_pandas()

label2id = {"O": 0, "B-PLAYER": 1}
id2label = {0: "O", 1: "B-PLAYER"}

def generate_tokens_and_tags(text, player_names, label2id):
    tokens = text.strip().split()
    ner_tags = ["O"] * len(tokens)

    for player in player_names:
        for i in range(len(tokens)):
            if tokens[i] == player:
                ner_tags[i] = "B-PLAYER"
                break

    return tokens, [label2id[tag] for tag in ner_tags]  # return lists

dataframe["tokens"] = None
dataframe["ner_tags"] = None

for index, row in dataframe.iterrows():
    tokens, ner_tags = generate_tokens_and_tags(row["Comment"], eval(row["Player Name In Commentary"]), label2id)
    dataframe.at[index, "tokens"] = tokens
    dataframe.at[index, "ner_tags"] = ner_tags

### Model creation

!pip install transformers datasets evaluate

from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split

# Split dataframe
train_df, test_df = train_test_split(dataframe[["tokens", "ner_tags"]], test_size=0.2, random_state=42)

# Convert to HuggingFace Dataset
dataset_dict = DatasetDict({
    "train": Dataset.from_pandas(train_df.reset_index(drop=True)),
    "test": Dataset.from_pandas(test_df.reset_index(drop=True)),
})

from transformers import AutoTokenizer

model_checkpoint = "bert-base-cased"  # or microsoft/MiniLM-L12-H384-uncased if you prefer
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def tokenize_and_align_labels(example):
    tokenized = tokenizer(
        example["tokens"],
        is_split_into_words=True,
        truncation=True,
        padding='max_length',
        max_length=128,
    )

    labels = []
    word_ids = tokenized.word_ids()
    previous_word_idx = None
    for word_idx in word_ids:
        if word_idx is None:
            labels.append(-100)
        elif word_idx != previous_word_idx:
            labels.append(example["ner_tags"][word_idx])
        else:
            labels.append(example["ner_tags"][word_idx])  # or -100 if you skip continuation
        previous_word_idx = word_idx

    tokenized["labels"] = labels
    return tokenized

tokenized_datasets = dataset_dict.map(tokenize_and_align_labels)


from transformers import AutoModelForTokenClassification

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id
)

# freeze all base model parameters
for name, param in model.base_model.named_parameters():
    param.requires_grad = False

# unfreeze base model pooling layers
found_pooler = False
for name, param in model.base_model.named_parameters():
    if "pooler" in name:
        param.requires_grad = True
        found_pooler = True
if not found_pooler:
    print("âš ï¸ No pooler layer found â€” all base layers remain frozen.")

import evaluate

accuracy = evaluate.load("accuracy")

def compute_metrics(p):
    predictions, labels = p
    predictions = predictions.argmax(-1)

    true_labels = [[l for l in label if l != -100] for label in labels]
    true_preds = [[p for p, l in zip(pred, label) if l != -100] for pred, label in zip(predictions, labels)]

    return accuracy.compute(
        predictions=[p for row in true_preds for p in row],
        references=[l for row in true_labels for l in row]
    )

from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification

training_args = TrainingArguments(
    output_dir="player-name-ner",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=10,
    weight_decay=0.01,
    logging_dir="./logs",
    load_best_model_at_end=True,
)

data_collator = DataCollatorForTokenClassification(tokenizer)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

trainer.model.cpu()
trainer.model.save_pretrained("/content/player-name-ner-model")
tokenizer.save_pretrained("/content/player-name-ner-model")

import shutil

shutil.make_archive("/content/player-name-ner-model", 'zip', "/content/player-name-ner-model")

dataset_dict['train'].to_pandas()[["tokens", "ner_tags"]]

ner_model = AutoModelForTokenClassification.from_pretrained("/content/player-name-ner-model")
tokenizer = AutoTokenizer.from_pretrained("/content/player-name-ner-model")

from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("player-name-ner-model")
model = AutoModelForTokenClassification.from_pretrained("player-name-ner-model")
label_map = model.config.id2label

# Input text
text = "Goal by Cristiano Ronaldo assisted by Vinicius Jr"
tokens = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
outputs = model(**tokens)
predictions = torch.argmax(outputs.logits, dim=2)

# Convert tokens + labels
tokens_list = tokenizer.convert_ids_to_tokens(tokens["input_ids"][0])
predicted_labels = [label_map[label.item()] for label in predictions[0]]

for token, label in zip(tokens_list, predicted_labels):
    print(f"{token:15} â†’ {label}")

## MP4 to WAV converter

!pip install moviepy

!pip install pydub

from pydub import AudioSegment
import os

def convert_to_wav_resample_pydub(input_path, output_path=None, target_sr=48000):
    # Load MP4 audio
    audio = AudioSegment.from_file(input_path, format="mp4")

    # Resample
    audio = audio.set_frame_rate(target_sr)

    # Optional: Set mono
    audio = audio.set_channels(1)

    if output_path is None:
        base = os.path.splitext(input_path)[0]
        output_path = base + ".wav"

    # Export as WAV
    audio.export(output_path, format="wav")
    print(f"âœ… Saved WAV @ {target_sr} Hz â†’ {output_path}")
    return output_path

convert_mp4_to_wav_resample_pydub("audio_samples/premier_league.mp4")

## Noise Filter

!pip install torch torchaudio -f https://download.pytorch.org/whl/cpu/torch_stable.html --quiet
!pip install deepfilternet --quiet

from df.enhance import enhance, init_df, load_audio, save_audio
from df.utils import download_file

if __name__ == "__main__":
    # Load default model
    model, df_state, _ = init_df()
    # Download and open some audio file. You use your audio files here
    audio_path = download_file(
        "https://github.com/Rikorose/DeepFilterNet/raw/e031053/assets/noisy_snr0.wav",
        download_dir=".",
    )
    audio, _ = load_audio(audio_path, sr=df_state.sr())
    # Denoise the audio
    enhanced = enhance(model, df_state, audio)
    # Save for listening
    save_audio("enhanced.wav", enhanced, df_state.sr())

### Enhance audio file

from df.enhance import enhance, init_df, load_audio, save_audio
from df.utils import download_file
import torch

def noise_filter(filepath):
    # Load default model
    model, df_state, _ = init_df()
    # Download and open some audio file. You use your audio files here
    audio_path = filepath
    base = os.path.splitext(audio_path)[0]
    output_path = base + ".wav"
    audio, _ = load_audio(audio_path, sr=df_state.sr())
    # audio_tensor = torch.tensor(audio, dtype=torch.float32)
    # Denoise the audio
    enhanced = enhance(model, df_state, audio)
    # Save for listening
    save_audio(output_path, enhanced, df_state.sr())

from df.enhance import enhance, init_df, load_audio, save_audio
from df.utils import download_file

if __name__ == "__main__":
    # Load default model
    model, df_state, _ = init_df()
    # Download and open some audio file. You use your audio files here
    audio_path = "/content/premier_league.wav"
    audio, _ = load_audio(audio_path, sr=df_state.sr())
    # Denoise the audio
    enhanced = enhance(model, df_state, audio)
    # Save for listening
    save_audio("enhanced.wav", enhanced, df_state.sr())

### Live enhancement

from df.enhance import enhance, init_df, load_audio
import torch
import time

model, df_state, _ = init_df()

torch.cuda.empty_cache()
def noise_filter_live(chunk):
    chunk_tensor = torch.tensor(chunk, dtype=torch.float32)
    enhanced = enhance(model, df_state, chunk_tensor)
    return enhanced.squeeze(0).numpy()

from df.enhance import enhance, init_df, load_audio, save_audio
from df.utils import download_file

if __name__ == "__main__":
    # Load default model
    model, df_state, _ = init_df()
    # Download and open some audio file. You use your audio files here
    audio_path = download_file(
        "https://github.com/Rikorose/DeepFilterNet/raw/e031053/assets/noisy_snr0.wav",
        download_dir=".",
    )
    audio, _ = load_audio(audio_path, sr=df_state.sr())
    # Denoise the audio
    enhanced = enhance(model, df_state, audio)
    # Save for listening
    save_audio("enhanced.wav", enhanced, df_state.sr())

!deepFilter /content/noisy_snr0.wav

### Live Enhancement with DEMUCS

!pip install -U demucs --quiet

!demucs --two-stems=vocals /content/premier_league.mp4

!pip install rnnoise --quiet



## Silero VAD based EOS

!pip install numpy==2.0.0

import torch
import numpy as np
import os
import shutil

# Load Silero VAD
vad_model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', trust_repo=True)
(get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils

# Constants
SAMPLING_RATE = 16000
CHUNK_SIZE = 512
silence_samples_threshold = SAMPLING_RATE // 2  # 0.5 sec silence = end of utterance

# Prepare output directory
CHUNK_ID = 0
utterance_buffer = []
silence_counter = 0

### VAD from live audio

# Save utterance chunk
def output_buffer():
    global utterance_buffer
    if len(utterance_buffer) > 0:
        audio_data = np.concatenate(utterance_buffer)
        audio_data_int16 = np.array(audio_data * 32768, dtype=np.int16)
        utterance_buffer.clear()

        return audio_data

# Process audio file
def process_audio_chunk(chunk_sample):

    # Iterate in chunks
    for i in range(0, len(chunk_sample), CHUNK_SIZE):
        chunk = chunk_sample[i:i + CHUNK_SIZE]
        chunk_tensor = chunk

        if len(chunk_tensor) < CHUNK_SIZE:
            break  # end of audio

        speech_prob = vad_model(chunk_tensor, SAMPLING_RATE).item()

        if speech_prob > 0.7:
            utterance_buffer.append(chunk)
            silence_counter = 0
            print(f"{i / SAMPLING_RATE:.2f}s - Speech detected")
        else:
            if len(utterance_buffer) > 0:
                silence_counter += len(chunk)
                if silence_counter >= silence_samples_threshold:
                    output_buffer()
                    silence_counter = 0
                else:
                    utterance_buffer.append(chunk)
                    print(f"{i / SAMPLING_RATE:.2f}s - Weak speech, added to buffer")

    # Final save if audio ends while speaking
    if len(utterance_buffer) > 0:
        output_buffer()

### Live audio simulation

import librosa
import base64
import numpy as np
import time
import IPython.display as ipd
from io import BytesIO
import soundfile as sf
import torchaudio

# JavaScript to stop all previous audio before playing the new one
js_code = f"""
var allAudios = document.getElementsByTagName('audio');
for (var i = 0; i < allAudios.length; i++) {{
    allAudios[i].pause();
    allAudios[i].currentTime = 0;
}}
"""
ipd.display(ipd.Javascript(js_code))  # Execute JS autoplay

# Load full audio
AUDIO_FILE = "/content/premier_league.wav"
SAMPLING_RATE = 48000
CHUNK_DURATION = 1  # in seconds

audio, sr = librosa.load(AUDIO_FILE, sr=SAMPLING_RATE)
chunk_size = int(sr * CHUNK_DURATION)
BUFFER = []

# Function to encode audio to base64
def encode_audio(audio_chunk, sr):
    buffer = BytesIO()
    sf.write(buffer, audio_chunk, sr, format="wav")
    base64_audio = base64.b64encode(buffer.getvalue()).decode("utf-8")
    return base64_audio

# Stream audio chunks
for start in range(0, len(audio), chunk_size):
    chunk = audio[start: start + chunk_size]

    # DeepFilterNet
    start = time.perf_counter()
    enhanced_chunk = noise_filter_live(chunk)
    stop = time.perf_counter()

    resampler = torchaudio.transforms.Resample(orig_freq=48000, new_freq=16000)
    waveform_16k = resampler(torch.tensor(enhanced_chunk, dtype=torch.float32))

    # Silero VAD
    vad_chunk = process_audio_chunk(waveform_16k)

    base64_audio = encode_audio(enhanced_chunk, 48000)

    # JavaScript to autoplay audio
    js_code = f"""
    var audio = new Audio("data:audio/wav;base64,{base64_audio}");
    audio.play();
    """
    ipd.display(ipd.Javascript(js_code))

    # Simulate real-time delay
    delay = (len(chunk) / sr)
    time.sleep(delay)

    print(stop - start)
    # print(f"ðŸ”Š Auto-playing chunk {start//chunk_size} ({delay:.2f} sec), encoding delay: {stop_enc - start_enc}, enhancement delay: {stop_enh - start_enh}, play delay: {stop_play - start_play}")


### VAD from audio file

shutil.rmtree('utterances', ignore_errors=True)
os.makedirs('utterances', exist_ok=True)

# Save utterance chunk
def save_utterance():
    global CHUNK_ID, utterance_buffer
    if len(utterance_buffer) > 1:
        audio_data = np.concatenate(utterance_buffer)
        audio_data_int16 = np.array(audio_data * 32768, dtype=np.int16)
        filename = f'utterances/utterance_{CHUNK_ID}.wav'
        write(filename, SAMPLING_RATE, audio_data_int16)
        print(f'[âœ… SAVED] {filename} (Length: {len(audio_data) / SAMPLING_RATE:.2f}s)')
        CHUNK_ID += 1
        utterance_buffer.clear()

# Process audio file
def process_audio_file(file_path):
    global utterance_buffer, silence_counter

    # Load audio file
    wav = read_audio(file_path, sampling_rate=SAMPLING_RATE)

    # Iterate in chunks
    for i in range(0, len(wav), CHUNK_SIZE):
        chunk = wav[i:i + CHUNK_SIZE]

        if len(chunk) < CHUNK_SIZE:
            break  # end of audio

        chunk_tensor = chunk
        speech_prob = vad_model(chunk_tensor, SAMPLING_RATE).item()

        if speech_prob > 0.8:
            utterance_buffer.append(chunk)
            silence_counter = 0
            print(f"{i / SAMPLING_RATE:.2f}s - Speech detected")
        else:
            if len(utterance_buffer) > 0:
                silence_counter += len(chunk)
                if silence_counter >= silence_samples_threshold:
                    save_utterance()
                    silence_counter = 0
                else:
                    utterance_buffer.append(chunk)
                    print(f"{i / SAMPLING_RATE:.2f}s - Weak speech, added to buffer")

    # Final save if audio ends while speaking
    if len(utterance_buffer) > 0:
        save_utterance()

process_audio_file('/content/audio_samples/premier_league_converted.wav')

## Faster-Whisper

!pip install faster-whisper

import os
import time
import shutil
from faster_whisper import WhisperModel

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
model_size = "distil-large-v3"

# Run on GPU with FP16
model = WhisperModel(model_size, device="cuda", compute_type="float16")

def stt(filename):
    global comments
    # print("Transcribing...")
    segments, info = model.transcribe(f'/content/drive/MyDrive/utterances/{filename}', language="en", vad_filter=False)
    # print("Transcribed")
    # print("Detected language '%s' with probability %f" % (info.language, info.language_probability))

    # print(segments[0].start, segments[-1].end, sep="->", end=": ")
    comment = ""
    for segment in segments:
        comment += str(segment.text).replace("\n", "") + " "

    return comment

comments = []
for filename in sorted(os.listdir('/content/drive/MyDrive/utterances')):
    comment = stt(filename)
    comments.extend([comment])

comments

## Data processing pipeline
